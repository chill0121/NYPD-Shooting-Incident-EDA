---
title: "NYPD Shooting Incident Data"
author: "Cody Hill"
date: "2023-04-04"
output:
  pdf_document: default
  html_document: default
---

## Setup

We will first begin by loading in the packages we intend to use. 

Then, importing the data using a URL directly from the source, ensures we will capture updates to the data as they come in, whenever this is run again.

```{r Setup RMD}
# Output all commands run and set a standard plot size
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 12, fig.height = 8) 

library(tidyverse)
library(lubridate)
library(ggmap)
library(ggplot2)

import_url <- read.csv("https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD")
```

## Transformation and Exploratory Data Analysis (EDA)

Let's take a look at the dimensions of this imported data.frame, as well as the variable types of each column, and summary.
```{r EDA}
dim(import_url)
str(import_url)
summary(import_url)
```

### Feature Removal and Renaming
Looks like we have 19 columns **(features)** and 25596 rows **(data points)**.  
First, let's remove any features that we won't be needing for our analysis.

1. `JURISDICTION_CODE` is pretty broad for localizing shooting incidents so we will end up using `BORO` instead which will give more insight to our analysis.
2. `X_COORD_CD`, `Y_COORD_CD`, and `Lon_Lat` are all redundant.

Also, let's rename a few of these for more readability.
```{r Feature Removal and Renaming}
import_url <- select(import_url, -JURISDICTION_CODE, -X_COORD_CD, -Y_COORD_CD, -Lon_Lat)
import_url <- import_url %>%
        rename(c('DATE' = 'OCCUR_DATE', 'TIME' = 'OCCUR_TIME','BOROUGH' = 'BORO', 
                'LOCATION' = 'LOCATION_DESC', 'MURDER_FLAG' = 'STATISTICAL_MURDER_FLAG', 
                'PERP_AGE' = 'PERP_AGE_GROUP', 'VICTIM_AGE' = 'VIC_AGE_GROUP', 'VICTIM_SEX' = 'VIC_SEX',
                'VICTIM_RACE' = 'VIC_RACE', 'LATITUDE' = 'Latitude', 'LONGITUDE' = 'Longitude'))
head(import_url)
```

### Check for Duplicates and Remove
Next, we will check if there are any missing or duplicated data points, focusing only on the `INCIDENT_KEY` feature for now.
This feature will be the most important for identifying any duplicate entries as they should all be unique.
```{r NA or Null and Duplicates}
# Check for any NA or Null values
any(is.na(import_url$INCIDENT_KEY)) | any(is.null(import_url$INCIDENT_KEY))
# Check for duplicates
length(unique(import_url$INCIDENT_KEY))
length(import_url$INCIDENT_KEY)
```
Subtracting the results here shows that there are **5470 duplicate data points**! 
Let's take a closer look to make sure these aren't false positives.
```{r Removal Duplicates}
# Query duplicates to see what they look like
head(filter(import_url, duplicated(import_url$INCIDENT_KEY)))
# Check a few entries
arrange(filter(import_url, INCIDENT_KEY == 227647476 | INCIDENT_KEY == 232390408), INCIDENT_KEY)
# Yes those are definitely duplicates
# Remove duplicates
import_url <- filter(import_url, !duplicated(import_url$INCIDENT_KEY))
# Check work
length(duplicated(import_url$INCIDENT_KEY))
```
That should do it for the duplicated data points. Let's continue our transformations.

### Change Feature Class Types
For better analysis we should change the class type of a few of these features to make them easier to work with.
```{r Class Changes}
# Character to Date and Period
import_url <- import_url %>% mutate(DATE = mdy(DATE)) %>%
        mutate(TIME = hms(TIME))

# Character to Factors - changes all character columns to factor
import_url <- import_url %>% mutate(across(where(is.character), as.factor))
str(import_url)
```

### More Feature Checks
We will continue to look at the features and see if any of these blank entries will cause trouble during the analysis.
Also, we'll look to see if there are any duplicate categorical factors in the rest of the features.
```{r More transformations}
# Create a table of each column to check factor levels
for(i in 1:length(import_url)){
    ifelse(is.factor(import_url[ ,i]), print(table(import_url[ ,i, drop = FALSE])), next)
}
```
Looks like there are quite a few blank entries and a few labeled as "UNKNOWN". 

- Over 50% of the `LOCATION` and `PERP_AGE` data is unknown.
- `BOROUGH`, `MURDER_FLAG`, `PRECINCT`, `DATE`, `TIME`, `LONGITUDE`, and `LATITUDE` have no missing entries.
- The remaining features are missing a few, but is not a significant amount compared to the overall size of the dataset.

We will combine these by labeling all blanks as "UNKNOWN". This data likely comes from officers on the scene and either
missed some information or, in the case of the perp, might not have had a witness or they didn't catch the offender. 
It's reasonable to leave these missing data points in because it isn't vital to the kind of analysis we will be doing, 
considering the data that is complete holds more relevance, removing it would be like throwing away the baby with the bathwater 
(e.g. removing a data point that is missing the `LOCATION` description, but isn't missing the rest of the information will 
only hurt our analysis. Especially considering `BOROUGH`,`LONGITUDE`, and`LATITUDE` aren't missing.)

Here we will combine and correct any missing entries with the methods discussed above.
```{r Dealing with the Unknown}
for(i in 1:length(import_url)){
    # IF the column is a factor AND contains empty space OR a 'U'
    if(is.factor(import_url[ ,i]) && '' %in% import_url[ ,i] || 'U' %in% import_url[ ,i]){
        # Add level named UNKNOWN
        levels(import_url[, i]) <- c(levels(import_url[, i]), 'UNKNOWN')
        # Replace missing values and Us with UNKNWON
        import_url[, i][import_url[, i] == ''] <- as.factor('UNKNOWN')
        import_url[, i][import_url[, i] == 'U'] <- as.factor('UNKNOWN')
        # Remove unused levels
        import_url[, i] <- droplevels(import_url[, i])
    }else{
        next
    }
}
```
Lastly there are a few values in `PERP_AGE` that look like data entry typos. 
See table above:('1020', '224', '940') 
Here, we cannot assume what was intended so we will change these age values to 'UNKNOWN'.
```{r PERP_AGE Anomalies}
# Before changes
table(import_url$PERP_AGE)
# Set values we want to keep as levels
age_levels <- c('<18', '18-24', '25-44', '45-64', '65+', 'UNKNOWN')
# Find all values NOT in age_levels (notice the !)
import_url$PERP_AGE[!import_url$PERP_AGE %in% age_levels] <- as.factor('UNKNOWN')
# Remove unused levels
import_url$PERP_AGE <- droplevels(import_url$PERP_AGE)
# After changes
table(import_url$PERP_AGE)
```

Let's take a look at a summary of this data now that we've cleaned it up.
```{r Summary}
summary(import_url)
```

### EDA Cont.
[Summarize some of the data]

## Visualizations

Now that the transformations are complete, let's start plotting these features against eachother and visualizing our data 
so we can try to make some conclusions.

Let's begin by visualizing the number of shooting incidents by the `BOROUGH` they occurred.
```{r Visualizations - Incidents per Borough}
# Order factor levels of BOROUGH based on frequency
import_url$BOROUGH <- fct_infreq(import_url$BOROUGH)
# Plot graph
import_url %>%
   ggplot(., aes(x = reorder(BOROUGH, BOROUGH, length, decreasing = TRUE), fill = BOROUGH)) + geom_bar(aes(y = after_stat(count))) +
   scale_fill_brewer(palette = 'YlOrRd', direction = -1) +
   labs(title = 'Number of Shooting Incidents by Borough',
        x = 'Borough', y = 'Number of Incidents',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
```
As you can see the majority of shooting incidents have occurred in Brooklyn folllowed by the Bronx and Queens.

Now to plot the number of shooting incidents organized by the `LOCATION` description assigned to them.
```{r Visualizations - Incidents per Location}
import_url %>%
   ggplot(., aes(x = reorder(LOCATION, LOCATION, length, decreasing = TRUE), fill = LOCATION)) + 
   geom_bar(aes(y = after_stat(count)), show.legend = FALSE) +
   theme(axis.text.x = element_text(angle = 90, vjust = 0.05)) +
   scale_y_log10() +
   labs(title = 'Number of Shooting Incidents by Location Description',
        x = 'Location Description', y = 'Number of Incidents (Log Scaled)',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
```
"Multi dwell-public housing" shows the most shooting incidents, followed by "Multi-dwell-apt building", and "Pvt house".
This graph shows evidence that a large portion of shooting incidents in NYC occur near the homes of those involved, and though we can't say for sure, 
likely isn't between complete strangers. This also shows just how many of these incidents were left without a location description. 
We don't know for certain of course, but this could have been left for many reasons -- a shooting was reported but 
they weren't sure exactly from where, on the street, etc.
Again, it is still important to look at the remaining data we do have as it is still quite a sizeable sample.

## Incident Coordinate Data Visualized on a Map

Here we're going to visualize the location of each shooting incident using the coordinates given in the dataset. 
First, we can use the minimum and maximum values of the longitudes and latitudes to find the map's bounding box (edges).
Then, use `ggmap()` to generate a map centered around these coordinates. Then, we can use `geom_point()` and `stat_density2d_filled()` to
superimpose our data on the map using the same coordinate system we generated.
```{r Map Visualization}
# Initialize the bounding box that will contain the map coordinates.
map_bounds <- c(left = min(import_url$LONGITUDE),
        bottom = min(import_url$LATITUDE),
        right = max(import_url$LONGITUDE),
        top = max(import_url$LATITUDE))

# Initialize the scatter plot of the incident coordinates
# Note, there are better maps out there but most require a private google API key,
# which wouldn't work for this public project.
incident_map_point <- ggmap(get_stamenmap(map_bounds, maptype = 'terrain', zoom = 11)) + 
        geom_point(data = import_url,
                aes(x = LONGITUDE, y = LATITUDE),
                color = 'darkred',
                size = 0.25,
                alpha = 0.2) +
        ggtitle('Point Plot of NYPD Shooting Incident Reporting 2006 - 2021') +
        labs(x = 'Longitude', y = 'Latitude',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
# Display Point Map
incident_map_point

# Initialize density map to better visualize regions with frequent incidents.
incident_map_density <- ggmap(get_stamenmap(map_bounds, maptype = 'terrain', zoom = 11)) + 
        stat_density2d_filled(data = import_url, contour_var = 'density',
                aes(x = LONGITUDE, y = LATITUDE, fill = after_stat(level)), 
                bins = 20,
                geom = 'polygon',
                alpha = 0.8) +
        geom_density_2d(data = import_url,
                aes(x = LONGITUDE, y = LATITUDE),
                bins = 20,
                alpha = 0.2,
                color = "white") +
        guides(fill = guide_legend(title = "Density")) +
        ggtitle('Density Plot of NYPD Shooting Incident Reporting 2006 - 2021') +
        labs(x = 'Longitude', y = 'Latitude', 
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
# Display Density Map
incident_map_density
```